{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_review",
      "provenance": [],
      "authorship_tag": "ABX9TyPTncwutYN5tJVne0plI5dg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irina-lebedeva/Pytorch-Review/blob/main/Pytorch_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBEGLn6GuLHT"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j1hg9xi6w7FX",
        "outputId": "066b9c4c-594b-4390-82b5-d6cf85f02f49"
      },
      "source": [
        "# normalize images in a batch\n",
        "\n",
        "def normalize_pic (A):\n",
        "     m = A.mean(dim = (2,3))\n",
        "     std = A.std(dim = (2,3))\n",
        "     A = (A - m[:,:, None, None]) /  std [:,:, None, None]\n",
        "     return A\n",
        "    \n",
        "A = torch.randint (0,256, (128, 3, 32, 32), dtype = torch.float32)\n",
        "normalize_pic(A)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-1.6516, -1.6516,  0.4162,  ..., -0.9979,  0.4696, -0.7311],\n",
              "          [ 0.5230, -0.8645,  1.4168,  ...,  0.7497,  0.5897, -1.3047],\n",
              "          [ 0.4296, -1.6116, -0.9579,  ...,  0.0427, -0.2108,  1.4835],\n",
              "          ...,\n",
              "          [-0.4776, -1.2380, -1.1313,  ...,  1.4701,  0.5363, -0.9178],\n",
              "          [-1.4381, -0.2775, -0.9579,  ...,  1.5769, -1.4648,  1.6036],\n",
              "          [ 0.0694,  1.3901, -1.3447,  ..., -0.8912, -1.0246,  1.4301]],\n",
              "\n",
              "         [[-0.7827, -1.3450, -1.7290,  ..., -0.7416,  0.1362,  0.8630],\n",
              "          [-0.5221,  0.1224,  0.9590,  ..., -0.6593,  1.7133, -1.7290],\n",
              "          [-0.3850,  0.0264, -1.3039,  ...,  1.3431, -1.6605, -0.5907],\n",
              "          ...,\n",
              "          [-0.6867, -0.2479, -1.7290,  ...,  1.5899, -0.6319,  0.5887],\n",
              "          [-1.3587,  1.4253, -1.1805,  ..., -1.0433, -1.3725,  1.4939],\n",
              "          [ 0.2459, -1.5782,  1.7133,  ...,  0.8905,  0.6847,  0.2184]],\n",
              "\n",
              "         [[-1.1710, -1.3184, -0.3130,  ..., -0.6750, -1.3989, -0.4471],\n",
              "          [-0.3666,  0.0221, -1.5865,  ..., -0.4873,  0.1964,  1.4833],\n",
              "          [-1.2782,  1.1884, -1.2648,  ..., -0.3264, -1.4659, -0.8760],\n",
              "          ...,\n",
              "          [-0.2192, -0.9163, -0.2326,  ..., -1.1173, -0.9297, -0.0583],\n",
              "          [-1.3318,  1.1348, -0.9297,  ...,  0.0087,  1.1750, -0.0315],\n",
              "          [ 0.9203, -0.8894, -0.3398,  ..., -0.5945,  1.5369,  0.5852]]],\n",
              "\n",
              "\n",
              "        [[[ 1.5631,  1.4134,  1.1412,  ..., -0.4651,  0.2427,  0.1883],\n",
              "          [-0.5740,  1.1820,  0.8281,  ...,  1.1820, -1.1321,  1.2773],\n",
              "          [ 0.2291, -0.1520, -0.9824,  ..., -0.7918,  0.5694, -0.9552],\n",
              "          ...,\n",
              "          [ 0.2563,  1.6448, -0.6965,  ...,  1.0186, -0.5060,  0.8281],\n",
              "          [ 0.1338,  0.8281,  0.5150,  ...,  0.8553, -0.3154, -1.0505],\n",
              "          [-0.6965,  1.2092, -0.4243,  ...,  0.1474, -0.1929, -1.0369]],\n",
              "\n",
              "         [[ 0.8473, -0.6747,  0.7793,  ..., -1.4221,  0.5211, -1.2862],\n",
              "          [-0.2127, -0.6339, -0.6475,  ...,  0.0591, -1.0416, -0.0224],\n",
              "          [-1.1911, -0.8650, -1.0144,  ..., -1.2998,  0.3037,  1.4995],\n",
              "          ...,\n",
              "          [ 0.5483, -1.1503, -0.7834,  ..., -0.6339,  1.2413,  0.0319],\n",
              "          [ 0.1270, -0.0089, -0.2127,  ...,  0.2493,  1.0239, -0.8921],\n",
              "          [-0.1719,  0.4668,  1.0375,  ..., -1.2998, -1.1775, -0.1583]],\n",
              "\n",
              "         [[ 1.2178,  1.3539, -0.8109,  ..., -1.0695, -1.6822, -1.4916],\n",
              "          [-0.8109, -0.7292, -1.6822,  ...,  0.2647, -1.7639, -1.0423],\n",
              "          [-1.2057, -1.0423, -1.2738,  ..., -1.2601,  1.1089, -0.4432],\n",
              "          ...,\n",
              "          [ 1.1906,  1.2450,  0.6732,  ..., -1.7775,  0.4145, -1.7231],\n",
              "          [ 1.0816,  0.5643, -0.6066,  ...,  0.8910, -0.3479, -0.2390],\n",
              "          [ 1.1769, -1.6005, -1.1376,  ...,  1.2450,  0.8774,  1.4084]]],\n",
              "\n",
              "\n",
              "        [[[-0.7101, -0.3537, -1.7382,  ..., -0.3400,  0.4139,  1.1678],\n",
              "          [-1.4640,  1.3871,  0.1397,  ..., -0.8609,  0.6606,  1.1129],\n",
              "          [-0.3400, -1.2858,  1.4282,  ..., -1.3818, -0.9980,  0.2631],\n",
              "          ...,\n",
              "          [-1.6970,  1.1404,  0.8662,  ...,  0.0027, -1.2447,  0.7566],\n",
              "          [-1.2584, -0.5319,  1.2363,  ...,  1.4419,  0.4550,  1.3323],\n",
              "          [ 1.2911,  0.9622, -1.2995,  ...,  1.2500, -1.3269, -0.0659]],\n",
              "\n",
              "         [[ 1.0187,  1.0879,  1.3509,  ..., -1.4038, -0.8916, -0.0333],\n",
              "          [ 0.9080,  0.3127,  0.4650,  ..., -1.5422, -0.2271,  0.4650],\n",
              "          [-1.3622,  1.0464,  0.4235,  ..., -1.3484,  0.9633,  1.1018],\n",
              "          ...,\n",
              "          [-0.9885, -1.3484, -1.7360,  ..., -0.1026,  1.7524,  1.6140],\n",
              "          [ 0.5065,  1.2264, -1.0300,  ..., -0.4763, -1.0162, -0.9885],\n",
              "          [ 1.6140, -0.3240, -0.4763,  ...,  0.4512,  0.4512, -1.4176]],\n",
              "\n",
              "         [[ 0.1100, -0.9307,  1.3150,  ..., -0.0132, -0.6705,  1.5615],\n",
              "          [ 0.6988,  1.7669, -1.6153,  ..., -0.1639,  0.8084, -0.9581],\n",
              "          [ 1.1096,  0.3976,  1.7669,  ..., -0.6294,  0.8084, -0.1776],\n",
              "          ...,\n",
              "          [-0.1639,  1.0685,  0.5208,  ..., -0.1776, -0.9855,  1.4245],\n",
              "          [ 0.5482, -1.4784,  1.5615,  ...,  0.8220,  1.0685,  0.6988],\n",
              "          [-0.9444,  0.8494, -0.5199,  ..., -1.2319,  0.3291, -0.4651]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[ 1.0035, -0.8253,  0.8409,  ..., -1.3808, -1.2182, -1.7330],\n",
              "          [ 0.7596,  0.6513, -0.5002,  ...,  0.2313,  0.9222,  0.0417],\n",
              "          [ 0.1229, -1.3672,  1.6266,  ..., -0.0532, -0.8931, -0.0125],\n",
              "          ...,\n",
              "          [ 1.5725, -1.2182, -0.8660,  ...,  0.2990, -1.1640, -1.2724],\n",
              "          [ 0.6242, -1.4079, -1.6652,  ...,  1.5725, -0.4867, -0.7982],\n",
              "          [-1.6246,  1.3286,  1.5047,  ...,  0.9493, -1.1234, -1.6652]],\n",
              "\n",
              "         [[-0.3107, -0.6346,  0.4179,  ...,  1.0386,  1.7133, -1.6196],\n",
              "          [ 0.0131,  0.6608, -1.2283,  ...,  1.2680, -1.6196, -0.0543],\n",
              "          [-0.9584,  1.5919,  1.3490,  ...,  1.6324, -0.6615,  1.5514],\n",
              "          ...,\n",
              "          [-0.0138, -1.5791, -0.5806,  ..., -1.6196,  1.0252,  1.7133],\n",
              "          [ 0.6473, -0.0138,  1.2141,  ...,  1.3355,  0.8362,  0.5799],\n",
              "          [ 0.7553,  0.9847,  1.0926,  ..., -1.4307,  0.4179,  1.7268]],\n",
              "\n",
              "         [[ 0.6585,  1.0888,  0.8528,  ...,  1.4636, -0.1049,  1.6718],\n",
              "          [-1.0210,  0.5752,  0.4087,  ..., -0.7156,  0.6863, -1.3263],\n",
              "          [ 0.1172, -0.7989, -0.2298,  ...,  1.6718, -0.0494,  0.6724],\n",
              "          ...,\n",
              "          [ 0.7973, -1.5762,  1.5468,  ...,  0.4087, -0.6878, -1.6178],\n",
              "          [-1.4790,  0.4920,  0.8667,  ...,  1.7550,  0.4781, -0.2159],\n",
              "          [-1.5901, -0.9932, -1.1875,  ...,  0.8667,  0.7557,  1.1443]]],\n",
              "\n",
              "\n",
              "        [[[-0.4132, -1.1442, -1.1580,  ..., -1.5855,  1.1729,  0.4971],\n",
              "          [ 1.2557,  0.3730,  0.9936,  ...,  0.7316, -1.0338,  0.4005],\n",
              "          [-1.4338, -1.0063, -1.3924,  ..., -0.0132,  0.4695, -1.4614],\n",
              "          ...,\n",
              "          [-1.6821, -1.0752,  0.9660,  ..., -0.0822, -0.5787, -1.2683],\n",
              "          [ 1.6694,  1.6970,  0.8557,  ...,  0.2212, -0.5373,  0.7453],\n",
              "          [-0.1787, -1.1856,  1.3108,  ..., -0.0132,  1.1315,  1.3798]],\n",
              "\n",
              "         [[-1.3245, -0.9429, -0.3705,  ..., -0.1524, -1.5290, -0.8748],\n",
              "          [ 1.2923, -0.7248, -0.0025,  ...,  0.6245, -0.9293, -1.6653],\n",
              "          [-0.2751,  0.2701,  0.0248,  ...,  1.1969, -1.5699, -0.4659],\n",
              "          ...,\n",
              "          [ 1.4967,  1.4150,  0.1202,  ...,  0.5018, -1.0519,  0.8425],\n",
              "          [-1.1746,  1.5785,  1.2242,  ...,  0.8153, -0.9565,  0.9107],\n",
              "          [ 0.8289, -0.1252, -0.0161,  ...,  1.6194, -1.3927, -1.2155]],\n",
              "\n",
              "         [[ 1.6908, -0.6467, -0.8629,  ..., -1.3493, -0.4440, -1.2412],\n",
              "          [-1.3223, -0.9034, -0.8359,  ..., -1.2953,  1.5287, -1.7006],\n",
              "          [-1.4169, -1.5385,  1.6503,  ..., -1.6330, -1.4574, -0.0927],\n",
              "          ...,\n",
              "          [-0.7278,  0.0559, -0.9304,  ..., -0.2278,  0.5693,  0.1775],\n",
              "          [ 0.0694,  0.0829, -0.4981,  ..., -1.1196, -0.1468,  0.0289],\n",
              "          [-0.2143, -0.2684, -1.6330,  ..., -1.2817,  0.8666, -0.2954]]],\n",
              "\n",
              "\n",
              "        [[[ 1.3771,  0.4714, -1.1776,  ..., -1.1641, -1.3128, -0.0692],\n",
              "          [-1.6913, -0.7856,  0.7012,  ...,  1.4312, -0.0963, -0.3261],\n",
              "          [ 0.5120, -0.7045,  1.5798,  ..., -1.5561, -0.5153,  0.1606],\n",
              "          ...,\n",
              "          [-0.3125,  0.9581,  0.7959,  ..., -1.1100,  0.6877, -1.1776],\n",
              "          [ 0.0254, -1.2587,  0.1065,  ...,  0.3498,  1.1743, -0.5288],\n",
              "          [ 1.1338, -1.1236,  1.3906,  ..., -1.2587,  0.7418,  1.4582]],\n",
              "\n",
              "         [[-0.8879,  0.4205,  0.9412,  ...,  0.3805,  0.9145, -1.4887],\n",
              "          [-1.0882,  1.5153,  0.4606,  ..., -0.7944,  1.4219,  0.8878],\n",
              "          [-0.5808, -0.2604, -0.8879,  ..., -1.4486,  1.1148,  0.2336],\n",
              "          ...,\n",
              "          [-0.6342,  1.6889,  0.0333,  ..., -1.0081, -1.5822,  0.8478],\n",
              "          [-1.2884,  0.4472, -0.3672,  ...,  1.3685, -1.2217, -1.4086],\n",
              "          [-0.2337, -1.1282,  1.7289,  ...,  1.2616, -1.1549, -1.5555]],\n",
              "\n",
              "         [[-1.4716,  0.3524,  1.2968,  ..., -0.0745, -0.1391, -0.8636],\n",
              "          [-0.9929, -1.6139,  0.7147,  ...,  1.1157,  1.5426, -0.4755],\n",
              "          [-1.6527,  0.1455, -0.7471,  ..., -1.0317, -0.3203, -1.5233],\n",
              "          ...,\n",
              "          [ 1.1157, -0.3720,  0.4171,  ...,  0.2619,  1.3485, -0.3720],\n",
              "          [-1.6656, -1.0059, -0.9283,  ..., -1.6268, -0.6695, -0.5919],\n",
              "          [ 0.6629,  1.5943, -0.4367,  ..., -0.6178,  1.6202,  1.1416]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix8plCZ-BThQ"
      },
      "source": [
        "# neuron example\n",
        "\n",
        "def forward_pass(x,w):\n",
        "    result  = x @ w\n",
        "    result = torch.sigmoid(result)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbN0Dh9PA6u_",
        "outputId": "ea389708-e332-4458-db49-e3d51034affc"
      },
      "source": [
        "x = torch.FloatTensor([[-5,-5], [2,-3], [1,-1]])\n",
        "w = torch.FloatTensor([[-0.5],[2.5]])\n",
        "result  = forward_pass(x,w)\n",
        "print('result: {}'.format(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result: tensor([[4.5398e-05],\n",
            "        [2.0343e-04],\n",
            "        [4.7426e-02]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZw2UYtCFhVA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "71640367-5573-44f2-9ea4-4b1b72b4a92c"
      },
      "source": [
        "# log. regression example\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston()\n",
        "plt.scatter(boston.data[:,-1], boston.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f93a597db50>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsKLJ4R_I3-L"
      },
      "source": [
        "w = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "x = torch.tensor(boston.data[:, -1], dtype  = torch.float32)\n",
        "y = torch.tensor(boston.target, dtype  = torch.float32)\n",
        "\n",
        "yhat  = w*x + b\n",
        "loss = torch.mean((y - yhat)**2)\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPAzYHikLGXh",
        "outputId": "be960798-d420-454d-94e6-cad300b8d404"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "for i in range(10):\n",
        "    yhat  = w*x + b\n",
        "    loss = torch.mean((y - yhat)**2)\n",
        "    loss.backward()\n",
        "\n",
        "    w.data -= 0.05 * w.grad.data\n",
        "    b.data -= 0.05 * b.grad.data\n",
        "\n",
        "    b.grad.data.zero_()\n",
        "    w.grad.data.zero_()\n",
        "\n",
        "    print('loss: {}'.format(loss))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 592.1469116210938\n",
            "loss: 456470.1875\n",
            "loss: 185682896.0\n",
            "loss: 75581251584.0\n",
            "loss: 30764997541888.0\n",
            "loss: 1.2522747024375808e+16\n",
            "loss: 5.097326010764886e+18\n",
            "loss: 2.0748435327016763e+21\n",
            "loss: 8.445555705499588e+23\n",
            "loss: 3.437724895399831e+26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "gxe9GqFgQkt5",
        "outputId": "27644ac9-fb6c-4d23-a000-6a4d6c47b5a2"
      },
      "source": [
        "# neural net\n",
        "\n",
        "N, D_in, H, D_out = 64, 3, 3, 10 \n",
        "\n",
        "x = torch.randn(N, D_in, dtype = torch.float)\n",
        "y = torch.randn(N, D_out, dtype = torch.float)\n",
        "w1 = torch.randn(D_in, H, dtype = torch.float, requires_grad= True)\n",
        "w2 = torch.randn(H, D_out, dtype = torch.float, requires_grad= True)\n",
        "\n",
        "\n",
        "yhat = (x @ w1).clamp(min=0).matmul(w2)\n",
        "\n",
        "loss = (y - yhat).pow(2).sum()\n",
        "loss.backward()\n",
        "\n",
        "w1 -= 0.01 * w1.grad.data\n",
        "w2 -= 0.01 * w2.grad.data\n",
        "w1.grad.data.zero_()\n",
        "w1.grad.data.zero_()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-db61e2b14faa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mw1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mw2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eHPZTGQVgRB"
      },
      "source": [
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as tfs\n",
        "\n",
        "data_tfs = tfs.Compose([tfs. ToTensor(),\n",
        "                        tfs. Normalize((0.5),(0.5))\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aezD9GlXVj7"
      },
      "source": [
        "root = './'\n",
        "train  = MNIST(root, train = True, transform = data_tfs, download = True)\n",
        "test  = MNIST(root, train = False, transform = data_tfs, download = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFPf_WeLZbsp"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size  = 128\n",
        "train_loader = DataLoader(train, batch_size, drop_last = True)\n",
        "test_loader = DataLoader(test, batch_size, drop_last = True)\n",
        "features= 784\n",
        "classes = 10\n",
        "\n",
        "epochs = 3\n",
        "history = []\n",
        "lr =0.001\n",
        "\n",
        "w = torch.FloatTensor(features, classes).uniform_(-1,1) / features**0.5\n",
        "w.requires_grad_()\n",
        "\n",
        "import numpy as np\n",
        "from torch.nn.functional import cross_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83bXxhjpe6FZ",
        "outputId": "210f2152-c3ec-4074-cdce-ae40df7a2837"
      },
      "source": [
        "for i in range(epochs):\n",
        "\n",
        "    for x_batch, y_batch in train_loader:\n",
        "        \n",
        "        x_batch = x_batch.reshape(x_batch.shape[0],-1)\n",
        "\n",
        "        logits = x_batch @ w \n",
        "\n",
        "        propabilities  = torch.exp(logits) / torch.exp(logits).sum(dim = 1, keepdims = True)\n",
        "        loss = -torch.log(propabilities[range(batch_size), y_batch]).mean()\n",
        "        history.append(loss.item())\n",
        "        loss.backward()\n",
        "        grad = w.grad\n",
        "        with torch.no_grad():\n",
        "            w -= lr*grad\n",
        "        w.grad.zero_()\n",
        "\n",
        "        print('loss: {}'.format(loss))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.4941470623016357\n",
            "loss: 2.4410054683685303\n",
            "loss: 2.4677581787109375\n",
            "loss: 2.4421119689941406\n",
            "loss: 2.437730312347412\n",
            "loss: 2.4433186054229736\n",
            "loss: 2.4255943298339844\n",
            "loss: 2.468085527420044\n",
            "loss: 2.3966965675354004\n",
            "loss: 2.3297078609466553\n",
            "loss: 2.388791561126709\n",
            "loss: 2.3664298057556152\n",
            "loss: 2.35149884223938\n",
            "loss: 2.3042330741882324\n",
            "loss: 2.3774023056030273\n",
            "loss: 2.312121868133545\n",
            "loss: 2.2758336067199707\n",
            "loss: 2.2717125415802\n",
            "loss: 2.348330020904541\n",
            "loss: 2.319132089614868\n",
            "loss: 2.30582332611084\n",
            "loss: 2.285341501235962\n",
            "loss: 2.27537202835083\n",
            "loss: 2.3198108673095703\n",
            "loss: 2.268178939819336\n",
            "loss: 2.294846534729004\n",
            "loss: 2.253688097000122\n",
            "loss: 2.258481025695801\n",
            "loss: 2.278045892715454\n",
            "loss: 2.2563517093658447\n",
            "loss: 2.269680976867676\n",
            "loss: 2.2276477813720703\n",
            "loss: 2.2012455463409424\n",
            "loss: 2.241727352142334\n",
            "loss: 2.2177159786224365\n",
            "loss: 2.191671133041382\n",
            "loss: 2.208951473236084\n",
            "loss: 2.2617380619049072\n",
            "loss: 2.206587553024292\n",
            "loss: 2.230236530303955\n",
            "loss: 2.1779541969299316\n",
            "loss: 2.16717529296875\n",
            "loss: 2.2407426834106445\n",
            "loss: 2.1908555030822754\n",
            "loss: 2.203002452850342\n",
            "loss: 2.196805715560913\n",
            "loss: 2.1677379608154297\n",
            "loss: 2.1534810066223145\n",
            "loss: 2.1786346435546875\n",
            "loss: 2.133974552154541\n",
            "loss: 2.1811602115631104\n",
            "loss: 2.1210219860076904\n",
            "loss: 2.1546294689178467\n",
            "loss: 2.223094940185547\n",
            "loss: 2.1194698810577393\n",
            "loss: 2.13316011428833\n",
            "loss: 2.188390016555786\n",
            "loss: 2.2063188552856445\n",
            "loss: 2.145289421081543\n",
            "loss: 2.133810520172119\n",
            "loss: 2.138225555419922\n",
            "loss: 2.082016706466675\n",
            "loss: 2.0725481510162354\n",
            "loss: 2.0306386947631836\n",
            "loss: 2.1435117721557617\n",
            "loss: 2.111643075942993\n",
            "loss: 2.03855037689209\n",
            "loss: 2.060511350631714\n",
            "loss: 2.091557025909424\n",
            "loss: 2.0734031200408936\n",
            "loss: 2.0643279552459717\n",
            "loss: 2.1013479232788086\n",
            "loss: 2.0848655700683594\n",
            "loss: 2.0915138721466064\n",
            "loss: 2.096757411956787\n",
            "loss: 2.0167038440704346\n",
            "loss: 2.0700228214263916\n",
            "loss: 2.0036487579345703\n",
            "loss: 2.097625494003296\n",
            "loss: 2.0373942852020264\n",
            "loss: 2.032630205154419\n",
            "loss: 2.0876729488372803\n",
            "loss: 2.0695085525512695\n",
            "loss: 2.080115795135498\n",
            "loss: 1.9921153783798218\n",
            "loss: 2.017866611480713\n",
            "loss: 1.9623956680297852\n",
            "loss: 2.0195999145507812\n",
            "loss: 1.9761126041412354\n",
            "loss: 2.01253080368042\n",
            "loss: 2.020803928375244\n",
            "loss: 1.987135887145996\n",
            "loss: 1.979728102684021\n",
            "loss: 2.0399482250213623\n",
            "loss: 1.989563226699829\n",
            "loss: 2.0006790161132812\n",
            "loss: 2.0397379398345947\n",
            "loss: 2.039444923400879\n",
            "loss: 2.06564998626709\n",
            "loss: 2.0041096210479736\n",
            "loss: 1.9818133115768433\n",
            "loss: 2.011007785797119\n",
            "loss: 1.975818157196045\n",
            "loss: 1.9974631071090698\n",
            "loss: 2.027280569076538\n",
            "loss: 1.8990027904510498\n",
            "loss: 1.9483897686004639\n",
            "loss: 1.9538192749023438\n",
            "loss: 1.9965882301330566\n",
            "loss: 2.008349657058716\n",
            "loss: 1.951728105545044\n",
            "loss: 1.9532302618026733\n",
            "loss: 1.9603790044784546\n",
            "loss: 2.0281882286071777\n",
            "loss: 2.0295987129211426\n",
            "loss: 1.9847078323364258\n",
            "loss: 1.8912274837493896\n",
            "loss: 1.8926103115081787\n",
            "loss: 1.9557805061340332\n",
            "loss: 1.9012150764465332\n",
            "loss: 1.964361310005188\n",
            "loss: 1.8887150287628174\n",
            "loss: 1.9371891021728516\n",
            "loss: 1.9982396364212036\n",
            "loss: 1.8852777481079102\n",
            "loss: 1.954595923423767\n",
            "loss: 1.8883464336395264\n",
            "loss: 1.9184964895248413\n",
            "loss: 1.8886052370071411\n",
            "loss: 1.8138008117675781\n",
            "loss: 1.9361037015914917\n",
            "loss: 1.939850091934204\n",
            "loss: 1.8898305892944336\n",
            "loss: 1.904686689376831\n",
            "loss: 1.9180270433425903\n",
            "loss: 1.906923532485962\n",
            "loss: 1.8957114219665527\n",
            "loss: 1.9833965301513672\n",
            "loss: 1.9435040950775146\n",
            "loss: 1.9350169897079468\n",
            "loss: 1.8676083087921143\n",
            "loss: 1.8190476894378662\n",
            "loss: 1.7982492446899414\n",
            "loss: 1.8799078464508057\n",
            "loss: 1.8701063394546509\n",
            "loss: 1.8714931011199951\n",
            "loss: 1.855867624282837\n",
            "loss: 1.8205291032791138\n",
            "loss: 1.8877891302108765\n",
            "loss: 1.8855741024017334\n",
            "loss: 1.7884509563446045\n",
            "loss: 1.7954695224761963\n",
            "loss: 1.7426226139068604\n",
            "loss: 1.7689710855484009\n",
            "loss: 1.744391918182373\n",
            "loss: 1.8225760459899902\n",
            "loss: 1.8850762844085693\n",
            "loss: 1.8011021614074707\n",
            "loss: 1.7894190549850464\n",
            "loss: 1.7245614528656006\n",
            "loss: 1.786572813987732\n",
            "loss: 1.7185513973236084\n",
            "loss: 1.7924468517303467\n",
            "loss: 1.8083055019378662\n",
            "loss: 1.7861907482147217\n",
            "loss: 1.7662047147750854\n",
            "loss: 1.7828763723373413\n",
            "loss: 1.8509857654571533\n",
            "loss: 1.816227912902832\n",
            "loss: 1.7991987466812134\n",
            "loss: 1.7292252779006958\n",
            "loss: 1.6184900999069214\n",
            "loss: 1.8600537776947021\n",
            "loss: 1.7962077856063843\n",
            "loss: 1.781714916229248\n",
            "loss: 1.8517720699310303\n",
            "loss: 1.876797080039978\n",
            "loss: 1.7336211204528809\n",
            "loss: 1.7366575002670288\n",
            "loss: 1.690650224685669\n",
            "loss: 1.7891205549240112\n",
            "loss: 1.6833269596099854\n",
            "loss: 1.666393518447876\n",
            "loss: 1.7654025554656982\n",
            "loss: 1.8092031478881836\n",
            "loss: 1.8077061176300049\n",
            "loss: 1.7377151250839233\n",
            "loss: 1.7151471376419067\n",
            "loss: 1.661993384361267\n",
            "loss: 1.7861099243164062\n",
            "loss: 1.7120682001113892\n",
            "loss: 1.7403655052185059\n",
            "loss: 1.7576801776885986\n",
            "loss: 1.7912732362747192\n",
            "loss: 1.7450780868530273\n",
            "loss: 1.6591371297836304\n",
            "loss: 1.7208874225616455\n",
            "loss: 1.709802269935608\n",
            "loss: 1.644312858581543\n",
            "loss: 1.63090181350708\n",
            "loss: 1.6448769569396973\n",
            "loss: 1.6463730335235596\n",
            "loss: 1.7115695476531982\n",
            "loss: 1.6263877153396606\n",
            "loss: 1.6747002601623535\n",
            "loss: 1.747323989868164\n",
            "loss: 1.752500057220459\n",
            "loss: 1.704810619354248\n",
            "loss: 1.6936588287353516\n",
            "loss: 1.6461809873580933\n",
            "loss: 1.6814918518066406\n",
            "loss: 1.6911234855651855\n",
            "loss: 1.708752989768982\n",
            "loss: 1.6978442668914795\n",
            "loss: 1.6937493085861206\n",
            "loss: 1.6136234998703003\n",
            "loss: 1.7194130420684814\n",
            "loss: 1.6745280027389526\n",
            "loss: 1.5625765323638916\n",
            "loss: 1.5537149906158447\n",
            "loss: 1.6062976121902466\n",
            "loss: 1.6062772274017334\n",
            "loss: 1.5814588069915771\n",
            "loss: 1.5633915662765503\n",
            "loss: 1.6291900873184204\n",
            "loss: 1.5455992221832275\n",
            "loss: 1.6742684841156006\n",
            "loss: 1.7530864477157593\n",
            "loss: 1.5959316492080688\n",
            "loss: 1.4918698072433472\n",
            "loss: 1.532293438911438\n",
            "loss: 1.640441656112671\n",
            "loss: 1.731926441192627\n",
            "loss: 1.7556209564208984\n",
            "loss: 1.6767216920852661\n",
            "loss: 1.7751795053482056\n",
            "loss: 1.5824759006500244\n",
            "loss: 1.6215895414352417\n",
            "loss: 1.6895774602890015\n",
            "loss: 1.7514560222625732\n",
            "loss: 1.655033826828003\n",
            "loss: 1.611055612564087\n",
            "loss: 1.5809037685394287\n",
            "loss: 1.5255681276321411\n",
            "loss: 1.7245510816574097\n",
            "loss: 1.6552951335906982\n",
            "loss: 1.6648682355880737\n",
            "loss: 1.7193794250488281\n",
            "loss: 1.6350164413452148\n",
            "loss: 1.6505563259124756\n",
            "loss: 1.6432569026947021\n",
            "loss: 1.6579575538635254\n",
            "loss: 1.6854585409164429\n",
            "loss: 1.6925318241119385\n",
            "loss: 1.6142650842666626\n",
            "loss: 1.617997646331787\n",
            "loss: 1.5481656789779663\n",
            "loss: 1.5498552322387695\n",
            "loss: 1.5792027711868286\n",
            "loss: 1.6094633340835571\n",
            "loss: 1.5976004600524902\n",
            "loss: 1.5409146547317505\n",
            "loss: 1.5352749824523926\n",
            "loss: 1.5453495979309082\n",
            "loss: 1.4509563446044922\n",
            "loss: 1.5239993333816528\n",
            "loss: 1.5910836458206177\n",
            "loss: 1.5024893283843994\n",
            "loss: 1.5280841588974\n",
            "loss: 1.5714552402496338\n",
            "loss: 1.5396498441696167\n",
            "loss: 1.5330140590667725\n",
            "loss: 1.6285011768341064\n",
            "loss: 1.5478285551071167\n",
            "loss: 1.6334203481674194\n",
            "loss: 1.6144015789031982\n",
            "loss: 1.4844030141830444\n",
            "loss: 1.5205183029174805\n",
            "loss: 1.5626188516616821\n",
            "loss: 1.4483145475387573\n",
            "loss: 1.5944597721099854\n",
            "loss: 1.6905163526535034\n",
            "loss: 1.513345718383789\n",
            "loss: 1.4364339113235474\n",
            "loss: 1.537265419960022\n",
            "loss: 1.5065985918045044\n",
            "loss: 1.418789029121399\n",
            "loss: 1.478113055229187\n",
            "loss: 1.4499337673187256\n",
            "loss: 1.5148004293441772\n",
            "loss: 1.423751950263977\n",
            "loss: 1.5330380201339722\n",
            "loss: 1.5569133758544922\n",
            "loss: 1.5521060228347778\n",
            "loss: 1.478805422782898\n",
            "loss: 1.510413408279419\n",
            "loss: 1.489490032196045\n",
            "loss: 1.526103138923645\n",
            "loss: 1.4983590841293335\n",
            "loss: 1.5546104907989502\n",
            "loss: 1.4449998140335083\n",
            "loss: 1.5636687278747559\n",
            "loss: 1.3788423538208008\n",
            "loss: 1.4316887855529785\n",
            "loss: 1.388506293296814\n",
            "loss: 1.3309805393218994\n",
            "loss: 1.4077117443084717\n",
            "loss: 1.5561447143554688\n",
            "loss: 1.451151728630066\n",
            "loss: 1.4191595315933228\n",
            "loss: 1.5848889350891113\n",
            "loss: 1.4936569929122925\n",
            "loss: 1.4694554805755615\n",
            "loss: 1.416686773300171\n",
            "loss: 1.483290195465088\n",
            "loss: 1.4698294401168823\n",
            "loss: 1.4978954792022705\n",
            "loss: 1.4713377952575684\n",
            "loss: 1.4682790040969849\n",
            "loss: 1.3734780550003052\n",
            "loss: 1.4794635772705078\n",
            "loss: 1.4804229736328125\n",
            "loss: 1.5547302961349487\n",
            "loss: 1.5953993797302246\n",
            "loss: 1.5572603940963745\n",
            "loss: 1.464409589767456\n",
            "loss: 1.5424209833145142\n",
            "loss: 1.432356834411621\n",
            "loss: 1.3769025802612305\n",
            "loss: 1.5243122577667236\n",
            "loss: 1.5506277084350586\n",
            "loss: 1.4952038526535034\n",
            "loss: 1.4049495458602905\n",
            "loss: 1.4106884002685547\n",
            "loss: 1.3323734998703003\n",
            "loss: 1.52731192111969\n",
            "loss: 1.6100059747695923\n",
            "loss: 1.3825788497924805\n",
            "loss: 1.2891006469726562\n",
            "loss: 1.388380765914917\n",
            "loss: 1.3483662605285645\n",
            "loss: 1.3917958736419678\n",
            "loss: 1.4699985980987549\n",
            "loss: 1.461238145828247\n",
            "loss: 1.497403860092163\n",
            "loss: 1.5345683097839355\n",
            "loss: 1.5129482746124268\n",
            "loss: 1.3496681451797485\n",
            "loss: 1.2992690801620483\n",
            "loss: 1.4180971384048462\n",
            "loss: 1.4887232780456543\n",
            "loss: 1.4119775295257568\n",
            "loss: 1.4026038646697998\n",
            "loss: 1.3259532451629639\n",
            "loss: 1.3750416040420532\n",
            "loss: 1.468245029449463\n",
            "loss: 1.3798632621765137\n",
            "loss: 1.4360798597335815\n",
            "loss: 1.5193188190460205\n",
            "loss: 1.4811642169952393\n",
            "loss: 1.376631736755371\n",
            "loss: 1.4852471351623535\n",
            "loss: 1.4570790529251099\n",
            "loss: 1.288156270980835\n",
            "loss: 1.3183958530426025\n",
            "loss: 1.302964687347412\n",
            "loss: 1.2858589887619019\n",
            "loss: 1.3483246564865112\n",
            "loss: 1.37990140914917\n",
            "loss: 1.4892590045928955\n",
            "loss: 1.3934623003005981\n",
            "loss: 1.417543888092041\n",
            "loss: 1.3049919605255127\n",
            "loss: 1.224123477935791\n",
            "loss: 1.403372049331665\n",
            "loss: 1.2629317045211792\n",
            "loss: 1.1889017820358276\n",
            "loss: 1.2884719371795654\n",
            "loss: 1.2385990619659424\n",
            "loss: 1.2942793369293213\n",
            "loss: 1.2552874088287354\n",
            "loss: 1.2896074056625366\n",
            "loss: 1.5189902782440186\n",
            "loss: 1.4244284629821777\n",
            "loss: 1.3919966220855713\n",
            "loss: 1.2175624370574951\n",
            "loss: 1.4618809223175049\n",
            "loss: 1.5443055629730225\n",
            "loss: 1.412489414215088\n",
            "loss: 1.4441660642623901\n",
            "loss: 1.4456814527511597\n",
            "loss: 1.3031790256500244\n",
            "loss: 1.3816183805465698\n",
            "loss: 1.4852631092071533\n",
            "loss: 1.3224765062332153\n",
            "loss: 1.3545501232147217\n",
            "loss: 1.3896441459655762\n",
            "loss: 1.2835745811462402\n",
            "loss: 1.230279564857483\n",
            "loss: 1.3340957164764404\n",
            "loss: 1.340235948562622\n",
            "loss: 1.3104223012924194\n",
            "loss: 1.239640474319458\n",
            "loss: 1.2996692657470703\n",
            "loss: 1.3328931331634521\n",
            "loss: 1.3304258584976196\n",
            "loss: 1.3394882678985596\n",
            "loss: 1.5055595636367798\n",
            "loss: 1.3668724298477173\n",
            "loss: 1.2139427661895752\n",
            "loss: 1.1700870990753174\n",
            "loss: 1.2901252508163452\n",
            "loss: 1.3899868726730347\n",
            "loss: 1.4053672552108765\n",
            "loss: 1.265998363494873\n",
            "loss: 1.3197972774505615\n",
            "loss: 1.2304847240447998\n",
            "loss: 1.2513368129730225\n",
            "loss: 1.3586775064468384\n",
            "loss: 1.3499095439910889\n",
            "loss: 1.249814510345459\n",
            "loss: 1.3652281761169434\n",
            "loss: 1.3684433698654175\n",
            "loss: 1.253720998764038\n",
            "loss: 1.271451473236084\n",
            "loss: 1.2301661968231201\n",
            "loss: 1.2314372062683105\n",
            "loss: 1.1620497703552246\n",
            "loss: 1.312087059020996\n",
            "loss: 1.4026525020599365\n",
            "loss: 1.1629271507263184\n",
            "loss: 1.2304141521453857\n",
            "loss: 1.2386130094528198\n",
            "loss: 1.3050971031188965\n",
            "loss: 1.1441757678985596\n",
            "loss: 1.1483497619628906\n",
            "loss: 1.2088289260864258\n",
            "loss: 1.16258704662323\n",
            "loss: 1.2316200733184814\n",
            "loss: 1.3279505968093872\n",
            "loss: 1.2417762279510498\n",
            "loss: 1.2662463188171387\n",
            "loss: 1.2800259590148926\n",
            "loss: 1.1637992858886719\n",
            "loss: 1.1800165176391602\n",
            "loss: 1.263464093208313\n",
            "loss: 1.1958184242248535\n",
            "loss: 1.3278334140777588\n",
            "loss: 1.2172523736953735\n",
            "loss: 1.3393117189407349\n",
            "loss: 1.2735623121261597\n",
            "loss: 1.295474886894226\n",
            "loss: 1.2445756196975708\n",
            "loss: 1.2152968645095825\n",
            "loss: 1.1105480194091797\n",
            "loss: 1.2087305784225464\n",
            "loss: 1.150989294052124\n",
            "loss: 1.2094993591308594\n",
            "loss: 1.2533154487609863\n",
            "loss: 1.2366445064544678\n",
            "loss: 0.986849308013916\n",
            "loss: 1.055547833442688\n",
            "loss: 1.060668706893921\n",
            "loss: 1.3010615110397339\n",
            "loss: 1.0802943706512451\n",
            "loss: 1.1060665845870972\n",
            "loss: 1.2552790641784668\n",
            "loss: 1.0149195194244385\n",
            "loss: 1.1248199939727783\n",
            "loss: 1.2776345014572144\n",
            "loss: 1.1269826889038086\n",
            "loss: 1.2660351991653442\n",
            "loss: 1.3649933338165283\n",
            "loss: 1.25690495967865\n",
            "loss: 1.2437818050384521\n",
            "loss: 1.3341296911239624\n",
            "loss: 1.4123681783676147\n",
            "loss: 1.1891906261444092\n",
            "loss: 1.2086728811264038\n",
            "loss: 1.2734216451644897\n",
            "loss: 1.2541382312774658\n",
            "loss: 1.084133267402649\n",
            "loss: 1.2122929096221924\n",
            "loss: 1.1543837785720825\n",
            "loss: 1.0399373769760132\n",
            "loss: 1.131223201751709\n",
            "loss: 1.2453579902648926\n",
            "loss: 1.240558385848999\n",
            "loss: 1.1255744695663452\n",
            "loss: 1.1787203550338745\n",
            "loss: 1.1131755113601685\n",
            "loss: 1.2537574768066406\n",
            "loss: 1.2304648160934448\n",
            "loss: 1.216327428817749\n",
            "loss: 1.2022886276245117\n",
            "loss: 1.228193998336792\n",
            "loss: 1.114107370376587\n",
            "loss: 1.190189003944397\n",
            "loss: 1.1560850143432617\n",
            "loss: 1.1852104663848877\n",
            "loss: 1.173099160194397\n",
            "loss: 1.1876534223556519\n",
            "loss: 1.2320818901062012\n",
            "loss: 1.0156850814819336\n",
            "loss: 1.140024185180664\n",
            "loss: 1.2685092687606812\n",
            "loss: 1.1352622509002686\n",
            "loss: 1.1782758235931396\n",
            "loss: 1.1418852806091309\n",
            "loss: 1.128925085067749\n",
            "loss: 1.089572787284851\n",
            "loss: 1.152748703956604\n",
            "loss: 1.1811813116073608\n",
            "loss: 1.2193400859832764\n",
            "loss: 1.130603313446045\n",
            "loss: 0.9498415589332581\n",
            "loss: 1.1051764488220215\n",
            "loss: 1.1112384796142578\n",
            "loss: 1.1629265546798706\n",
            "loss: 0.9783098697662354\n",
            "loss: 1.1742355823516846\n",
            "loss: 1.3352785110473633\n",
            "loss: 1.1497559547424316\n",
            "loss: 1.2056971788406372\n",
            "loss: 1.403904914855957\n",
            "loss: 1.3324849605560303\n",
            "loss: 1.251757025718689\n",
            "loss: 1.189034104347229\n",
            "loss: 1.2735143899917603\n",
            "loss: 1.1345640420913696\n",
            "loss: 1.1174482107162476\n",
            "loss: 1.0884448289871216\n",
            "loss: 1.2966220378875732\n",
            "loss: 1.206279993057251\n",
            "loss: 1.0281262397766113\n",
            "loss: 1.1200319528579712\n",
            "loss: 1.2611020803451538\n",
            "loss: 1.2520854473114014\n",
            "loss: 1.0303475856781006\n",
            "loss: 1.1756761074066162\n",
            "loss: 1.157656192779541\n",
            "loss: 1.1354438066482544\n",
            "loss: 1.176066517829895\n",
            "loss: 1.049049735069275\n",
            "loss: 1.1078442335128784\n",
            "loss: 1.042115330696106\n",
            "loss: 1.1307380199432373\n",
            "loss: 1.1340422630310059\n",
            "loss: 1.0445656776428223\n",
            "loss: 1.050355076789856\n",
            "loss: 1.0545457601547241\n",
            "loss: 1.1706584692001343\n",
            "loss: 1.06816828250885\n",
            "loss: 1.0512992143630981\n",
            "loss: 1.007663607597351\n",
            "loss: 1.1341897249221802\n",
            "loss: 1.0521794557571411\n",
            "loss: 1.0759226083755493\n",
            "loss: 1.2515265941619873\n",
            "loss: 1.2069870233535767\n",
            "loss: 1.1368354558944702\n",
            "loss: 1.1754834651947021\n",
            "loss: 1.069413423538208\n",
            "loss: 1.1516810655593872\n",
            "loss: 1.195918321609497\n",
            "loss: 1.1956498622894287\n",
            "loss: 1.240030288696289\n",
            "loss: 1.1664713621139526\n",
            "loss: 1.1270232200622559\n",
            "loss: 1.2332260608673096\n",
            "loss: 1.1835342645645142\n",
            "loss: 1.1385506391525269\n",
            "loss: 1.163535475730896\n",
            "loss: 0.9541064500808716\n",
            "loss: 1.1225636005401611\n",
            "loss: 1.1924349069595337\n",
            "loss: 1.1983857154846191\n",
            "loss: 1.2311065196990967\n",
            "loss: 1.1409605741500854\n",
            "loss: 1.1786421537399292\n",
            "loss: 1.2274518013000488\n",
            "loss: 1.2398878335952759\n",
            "loss: 1.3783077001571655\n",
            "loss: 1.3091871738433838\n",
            "loss: 1.0494120121002197\n",
            "loss: 1.0151727199554443\n",
            "loss: 1.173094391822815\n",
            "loss: 1.0669634342193604\n",
            "loss: 1.1583318710327148\n",
            "loss: 1.0633403062820435\n",
            "loss: 1.1629953384399414\n",
            "loss: 1.3035047054290771\n",
            "loss: 1.1214145421981812\n",
            "loss: 1.2071340084075928\n",
            "loss: 1.0877972841262817\n",
            "loss: 1.068389892578125\n",
            "loss: 1.0299004316329956\n",
            "loss: 0.9928488731384277\n",
            "loss: 1.1560248136520386\n",
            "loss: 1.2551984786987305\n",
            "loss: 1.1779758930206299\n",
            "loss: 1.140138864517212\n",
            "loss: 1.1706888675689697\n",
            "loss: 1.1347651481628418\n",
            "loss: 1.1617904901504517\n",
            "loss: 1.313502550125122\n",
            "loss: 1.2860933542251587\n",
            "loss: 1.2098278999328613\n",
            "loss: 1.0757534503936768\n",
            "loss: 1.0185692310333252\n",
            "loss: 0.992823600769043\n",
            "loss: 1.1288056373596191\n",
            "loss: 1.0997486114501953\n",
            "loss: 1.0920778512954712\n",
            "loss: 1.0496327877044678\n",
            "loss: 1.027801275253296\n",
            "loss: 1.159407377243042\n",
            "loss: 1.1957777738571167\n",
            "loss: 1.0266492366790771\n",
            "loss: 1.007358431816101\n",
            "loss: 1.0159322023391724\n",
            "loss: 0.963181734085083\n",
            "loss: 0.9393209218978882\n",
            "loss: 1.1027461290359497\n",
            "loss: 1.2017581462860107\n",
            "loss: 1.1224174499511719\n",
            "loss: 1.09955894947052\n",
            "loss: 0.9754651188850403\n",
            "loss: 1.0261731147766113\n",
            "loss: 1.0042099952697754\n",
            "loss: 1.0692471265792847\n",
            "loss: 1.1804181337356567\n",
            "loss: 1.0622122287750244\n",
            "loss: 0.9867271184921265\n",
            "loss: 1.0417330265045166\n",
            "loss: 1.112977385520935\n",
            "loss: 1.1009109020233154\n",
            "loss: 1.089781641960144\n",
            "loss: 0.892021656036377\n",
            "loss: 0.8601926565170288\n",
            "loss: 1.1872591972351074\n",
            "loss: 1.1397640705108643\n",
            "loss: 1.0498595237731934\n",
            "loss: 1.2555923461914062\n",
            "loss: 1.2643789052963257\n",
            "loss: 1.024674415588379\n",
            "loss: 1.0315850973129272\n",
            "loss: 0.9497312903404236\n",
            "loss: 1.0970473289489746\n",
            "loss: 0.9918708205223083\n",
            "loss: 0.9612555503845215\n",
            "loss: 1.0623446702957153\n",
            "loss: 1.132315754890442\n",
            "loss: 1.1523823738098145\n",
            "loss: 1.0626747608184814\n",
            "loss: 1.0373550653457642\n",
            "loss: 0.9450848698616028\n",
            "loss: 1.1696830987930298\n",
            "loss: 1.0299525260925293\n",
            "loss: 1.0882412195205688\n",
            "loss: 1.1012617349624634\n",
            "loss: 1.1968215703964233\n",
            "loss: 1.1037206649780273\n",
            "loss: 0.9942688941955566\n",
            "loss: 1.0494060516357422\n",
            "loss: 1.083802580833435\n",
            "loss: 0.9123147130012512\n",
            "loss: 0.9445877075195312\n",
            "loss: 0.933975100517273\n",
            "loss: 1.0425591468811035\n",
            "loss: 1.0873587131500244\n",
            "loss: 0.9435002207756042\n",
            "loss: 0.988222062587738\n",
            "loss: 1.1019374132156372\n",
            "loss: 1.1784096956253052\n",
            "loss: 1.117002010345459\n",
            "loss: 1.1225359439849854\n",
            "loss: 1.0460169315338135\n",
            "loss: 1.0234037637710571\n",
            "loss: 1.0410352945327759\n",
            "loss: 1.0993984937667847\n",
            "loss: 1.0722957849502563\n",
            "loss: 1.0951110124588013\n",
            "loss: 0.9563001990318298\n",
            "loss: 1.0885376930236816\n",
            "loss: 1.0663686990737915\n",
            "loss: 0.8859861493110657\n",
            "loss: 0.8987765312194824\n",
            "loss: 0.9927370548248291\n",
            "loss: 1.0144280195236206\n",
            "loss: 0.9701674580574036\n",
            "loss: 0.9998470544815063\n",
            "loss: 1.0185459852218628\n",
            "loss: 0.9244541525840759\n",
            "loss: 1.0308644771575928\n",
            "loss: 1.2087312936782837\n",
            "loss: 0.9933502674102783\n",
            "loss: 0.8819675445556641\n",
            "loss: 0.9302881360054016\n",
            "loss: 1.0433357954025269\n",
            "loss: 1.1913042068481445\n",
            "loss: 1.271232008934021\n",
            "loss: 1.1337144374847412\n",
            "loss: 1.3022761344909668\n",
            "loss: 0.9924426674842834\n",
            "loss: 1.0414352416992188\n",
            "loss: 1.1430466175079346\n",
            "loss: 1.225345253944397\n",
            "loss: 1.103912353515625\n",
            "loss: 1.0764800310134888\n",
            "loss: 1.0146255493164062\n",
            "loss: 0.9787433743476868\n",
            "loss: 1.2410364151000977\n",
            "loss: 1.1274811029434204\n",
            "loss: 1.0751681327819824\n",
            "loss: 1.2353765964508057\n",
            "loss: 1.0826667547225952\n",
            "loss: 1.090643286705017\n",
            "loss: 1.0552523136138916\n",
            "loss: 1.169400691986084\n",
            "loss: 1.1811277866363525\n",
            "loss: 1.227277398109436\n",
            "loss: 1.0121289491653442\n",
            "loss: 1.05302095413208\n",
            "loss: 1.0053043365478516\n",
            "loss: 0.9860086441040039\n",
            "loss: 1.0605281591415405\n",
            "loss: 1.0539458990097046\n",
            "loss: 1.0768412351608276\n",
            "loss: 1.0179541110992432\n",
            "loss: 0.9739541411399841\n",
            "loss: 0.9695272445678711\n",
            "loss: 0.8325181007385254\n",
            "loss: 0.9529193639755249\n",
            "loss: 1.03536856174469\n",
            "loss: 0.909334659576416\n",
            "loss: 0.9780716896057129\n",
            "loss: 1.0608652830123901\n",
            "loss: 1.0112055540084839\n",
            "loss: 1.0357047319412231\n",
            "loss: 1.1293803453445435\n",
            "loss: 0.9675944447517395\n",
            "loss: 1.0822194814682007\n",
            "loss: 1.0487662553787231\n",
            "loss: 0.9527355432510376\n",
            "loss: 0.9807820916175842\n",
            "loss: 1.0583250522613525\n",
            "loss: 0.9180181622505188\n",
            "loss: 1.0708848237991333\n",
            "loss: 1.2144101858139038\n",
            "loss: 0.963009238243103\n",
            "loss: 0.8717474937438965\n",
            "loss: 1.0408592224121094\n",
            "loss: 0.9830354452133179\n",
            "loss: 0.842087984085083\n",
            "loss: 0.969338595867157\n",
            "loss: 0.9142934083938599\n",
            "loss: 0.999343752861023\n",
            "loss: 0.9089314937591553\n",
            "loss: 1.0428025722503662\n",
            "loss: 1.0875773429870605\n",
            "loss: 1.0530368089675903\n",
            "loss: 0.9898735284805298\n",
            "loss: 1.0306049585342407\n",
            "loss: 0.9855754375457764\n",
            "loss: 1.018537998199463\n",
            "loss: 0.9969950318336487\n",
            "loss: 1.0654557943344116\n",
            "loss: 0.9415913224220276\n",
            "loss: 1.081634759902954\n",
            "loss: 0.8727238774299622\n",
            "loss: 0.9100361466407776\n",
            "loss: 0.8824473023414612\n",
            "loss: 0.8006167411804199\n",
            "loss: 0.8909378051757812\n",
            "loss: 1.1251118183135986\n",
            "loss: 1.0084738731384277\n",
            "loss: 0.9561371803283691\n",
            "loss: 1.1615517139434814\n",
            "loss: 1.06449294090271\n",
            "loss: 0.9706358313560486\n",
            "loss: 0.9316157698631287\n",
            "loss: 0.991357684135437\n",
            "loss: 0.9441128373146057\n",
            "loss: 1.0207034349441528\n",
            "loss: 0.9926390051841736\n",
            "loss: 0.9713190197944641\n",
            "loss: 0.8672861456871033\n",
            "loss: 1.0174405574798584\n",
            "loss: 0.9879587888717651\n",
            "loss: 1.1122827529907227\n",
            "loss: 1.1842600107192993\n",
            "loss: 1.1340675354003906\n",
            "loss: 1.000680923461914\n",
            "loss: 1.0882291793823242\n",
            "loss: 0.9553618431091309\n",
            "loss: 0.9003326296806335\n",
            "loss: 1.1170655488967896\n",
            "loss: 1.1478497982025146\n",
            "loss: 1.0541276931762695\n",
            "loss: 0.9375931620597839\n",
            "loss: 0.9405964612960815\n",
            "loss: 0.8853771686553955\n",
            "loss: 1.103408932685852\n",
            "loss: 1.1941914558410645\n",
            "loss: 0.9045009613037109\n",
            "loss: 0.7816473245620728\n",
            "loss: 0.8742838501930237\n",
            "loss: 0.8661254048347473\n",
            "loss: 0.9277083873748779\n",
            "loss: 1.0296627283096313\n",
            "loss: 1.0255115032196045\n",
            "loss: 1.0652108192443848\n",
            "loss: 1.0955568552017212\n",
            "loss: 1.0924121141433716\n",
            "loss: 0.8793447017669678\n",
            "loss: 0.8160476684570312\n",
            "loss: 0.9642842411994934\n",
            "loss: 1.0722697973251343\n",
            "loss: 0.9765752553939819\n",
            "loss: 1.0019235610961914\n",
            "loss: 0.8724135160446167\n",
            "loss: 0.923306405544281\n",
            "loss: 1.0678833723068237\n",
            "loss: 0.946327805519104\n",
            "loss: 0.9888916015625\n",
            "loss: 1.0843884944915771\n",
            "loss: 1.0811067819595337\n",
            "loss: 0.9617717862129211\n",
            "loss: 1.0732388496398926\n",
            "loss: 1.0407222509384155\n",
            "loss: 0.8494582772254944\n",
            "loss: 0.8764614462852478\n",
            "loss: 0.8620389103889465\n",
            "loss: 0.8377485275268555\n",
            "loss: 0.9193889498710632\n",
            "loss: 0.9515863656997681\n",
            "loss: 1.092159628868103\n",
            "loss: 0.9990750551223755\n",
            "loss: 0.9981091618537903\n",
            "loss: 0.8830831050872803\n",
            "loss: 0.7926280498504639\n",
            "loss: 1.0040347576141357\n",
            "loss: 0.8187718391418457\n",
            "loss: 0.7460758686065674\n",
            "loss: 0.891277551651001\n",
            "loss: 0.8016014099121094\n",
            "loss: 0.8803057074546814\n",
            "loss: 0.8480573296546936\n",
            "loss: 0.8531914353370667\n",
            "loss: 1.175662636756897\n",
            "loss: 1.0347607135772705\n",
            "loss: 1.012671947479248\n",
            "loss: 0.7916297316551208\n",
            "loss: 1.0999619960784912\n",
            "loss: 1.192034125328064\n",
            "loss: 0.9822428226470947\n",
            "loss: 1.0535181760787964\n",
            "loss: 1.0586885213851929\n",
            "loss: 0.8916933536529541\n",
            "loss: 0.9676558375358582\n",
            "loss: 1.1738011837005615\n",
            "loss: 0.9291391372680664\n",
            "loss: 0.9773018956184387\n",
            "loss: 0.9884704947471619\n",
            "loss: 0.8739825487136841\n",
            "loss: 0.8078072667121887\n",
            "loss: 0.9368664026260376\n",
            "loss: 0.9759786128997803\n",
            "loss: 0.8726339340209961\n",
            "loss: 0.8093900084495544\n",
            "loss: 0.8671749234199524\n",
            "loss: 0.9322274327278137\n",
            "loss: 0.921037495136261\n",
            "loss: 0.9784775972366333\n",
            "loss: 1.1776913404464722\n",
            "loss: 0.9827519655227661\n",
            "loss: 0.8288466334342957\n",
            "loss: 0.73566734790802\n",
            "loss: 0.9186052680015564\n",
            "loss: 1.0265365839004517\n",
            "loss: 1.0701364278793335\n",
            "loss: 0.8711048364639282\n",
            "loss: 0.9442469477653503\n",
            "loss: 0.8411179184913635\n",
            "loss: 0.8667771220207214\n",
            "loss: 1.0053054094314575\n",
            "loss: 0.9309602975845337\n",
            "loss: 0.848048210144043\n",
            "loss: 1.004737138748169\n",
            "loss: 1.0053659677505493\n",
            "loss: 0.8576103448867798\n",
            "loss: 0.8771829009056091\n",
            "loss: 0.8450434803962708\n",
            "loss: 0.8590152859687805\n",
            "loss: 0.7687058448791504\n",
            "loss: 0.9548203945159912\n",
            "loss: 1.0759975910186768\n",
            "loss: 0.7650832533836365\n",
            "loss: 0.8311837911605835\n",
            "loss: 0.8535165786743164\n",
            "loss: 0.9210085868835449\n",
            "loss: 0.8018919229507446\n",
            "loss: 0.7796798944473267\n",
            "loss: 0.8243552446365356\n",
            "loss: 0.7538986206054688\n",
            "loss: 0.8525933027267456\n",
            "loss: 0.96038419008255\n",
            "loss: 0.8612940311431885\n",
            "loss: 0.9141677618026733\n",
            "loss: 0.9286063313484192\n",
            "loss: 0.8017668128013611\n",
            "loss: 0.8127433061599731\n",
            "loss: 0.9265676736831665\n",
            "loss: 0.8139770030975342\n",
            "loss: 0.993475079536438\n",
            "loss: 0.8616081476211548\n",
            "loss: 0.9875953197479248\n",
            "loss: 0.9349603652954102\n",
            "loss: 0.9255895614624023\n",
            "loss: 0.8725966215133667\n",
            "loss: 0.8483734726905823\n",
            "loss: 0.7202285528182983\n",
            "loss: 0.8140969276428223\n",
            "loss: 0.7956168055534363\n",
            "loss: 0.8356003761291504\n",
            "loss: 0.8775087594985962\n",
            "loss: 0.8565428853034973\n",
            "loss: 0.5922262072563171\n",
            "loss: 0.6579648852348328\n",
            "loss: 0.6807630062103271\n",
            "loss: 0.9612749814987183\n",
            "loss: 0.7220829129219055\n",
            "loss: 0.7149256467819214\n",
            "loss: 0.9364356994628906\n",
            "loss: 0.6159005761146545\n",
            "loss: 0.7985524535179138\n",
            "loss: 0.95558100938797\n",
            "loss: 0.7628844976425171\n",
            "loss: 0.9452846050262451\n",
            "loss: 1.0427285432815552\n",
            "loss: 0.9143053889274597\n",
            "loss: 0.9217424988746643\n",
            "loss: 1.01790452003479\n",
            "loss: 1.126591682434082\n",
            "loss: 0.8626912832260132\n",
            "loss: 0.9106405377388\n",
            "loss: 0.9370551109313965\n",
            "loss: 0.9222306609153748\n",
            "loss: 0.7447391748428345\n",
            "loss: 0.8694728016853333\n",
            "loss: 0.8301596641540527\n",
            "loss: 0.7164028882980347\n",
            "loss: 0.7989670038223267\n",
            "loss: 0.9216212034225464\n",
            "loss: 0.9102275371551514\n",
            "loss: 0.7954168319702148\n",
            "loss: 0.8688914775848389\n",
            "loss: 0.8026068806648254\n",
            "loss: 0.928500771522522\n",
            "loss: 0.8927778601646423\n",
            "loss: 0.8857048153877258\n",
            "loss: 0.8981329202651978\n",
            "loss: 0.9168053269386292\n",
            "loss: 0.7863171100616455\n",
            "loss: 0.8790732622146606\n",
            "loss: 0.8196043968200684\n",
            "loss: 0.8675017952919006\n",
            "loss: 0.8749894499778748\n",
            "loss: 0.8782960772514343\n",
            "loss: 0.9116227626800537\n",
            "loss: 0.6964777708053589\n",
            "loss: 0.8383210897445679\n",
            "loss: 0.9585046768188477\n",
            "loss: 0.8425796627998352\n",
            "loss: 0.8770899772644043\n",
            "loss: 0.8602737188339233\n",
            "loss: 0.840509831905365\n",
            "loss: 0.7614161968231201\n",
            "loss: 0.8537716865539551\n",
            "loss: 0.8649368286132812\n",
            "loss: 0.8992428183555603\n",
            "loss: 0.8192204236984253\n",
            "loss: 0.6471346616744995\n",
            "loss: 0.7929277420043945\n",
            "loss: 0.7997947335243225\n",
            "loss: 0.8480211496353149\n",
            "loss: 0.6621072888374329\n",
            "loss: 0.840023934841156\n",
            "loss: 1.038498878479004\n",
            "loss: 0.8850337266921997\n",
            "loss: 0.8915003538131714\n",
            "loss: 1.1302947998046875\n",
            "loss: 1.0297887325286865\n",
            "loss: 0.9546571373939514\n",
            "loss: 0.8888591527938843\n",
            "loss: 0.9833105802536011\n",
            "loss: 0.8826848268508911\n",
            "loss: 0.8448601365089417\n",
            "loss: 0.8157969117164612\n",
            "loss: 1.0317115783691406\n",
            "loss: 0.9152932167053223\n",
            "loss: 0.7637894153594971\n",
            "loss: 0.8590754270553589\n",
            "loss: 1.0154542922973633\n",
            "loss: 1.0139265060424805\n",
            "loss: 0.7335492372512817\n",
            "loss: 0.8771178722381592\n",
            "loss: 0.8842891454696655\n",
            "loss: 0.8537113666534424\n",
            "loss: 0.8828402757644653\n",
            "loss: 0.7606545686721802\n",
            "loss: 0.8225094676017761\n",
            "loss: 0.756710410118103\n",
            "loss: 0.8281930088996887\n",
            "loss: 0.8599884510040283\n",
            "loss: 0.7784011960029602\n",
            "loss: 0.7329584360122681\n",
            "loss: 0.7344734072685242\n",
            "loss: 0.8800500631332397\n",
            "loss: 0.7878406643867493\n",
            "loss: 0.7613252401351929\n",
            "loss: 0.7375056743621826\n",
            "loss: 0.8670950531959534\n",
            "loss: 0.7733891606330872\n",
            "loss: 0.7781026363372803\n",
            "loss: 1.0183887481689453\n",
            "loss: 0.9680161476135254\n",
            "loss: 0.8533546924591064\n",
            "loss: 0.8832103610038757\n",
            "loss: 0.7950706481933594\n",
            "loss: 0.8865303993225098\n",
            "loss: 0.91407310962677\n",
            "loss: 0.9234983325004578\n",
            "loss: 0.9919998645782471\n",
            "loss: 0.900076687335968\n",
            "loss: 0.8420378565788269\n",
            "loss: 0.9981837272644043\n",
            "loss: 0.9474164843559265\n",
            "loss: 0.8637452721595764\n",
            "loss: 0.8712577223777771\n",
            "loss: 0.6849501729011536\n",
            "loss: 0.8625696301460266\n",
            "loss: 0.9312587976455688\n",
            "loss: 0.9365534782409668\n",
            "loss: 0.9831197261810303\n",
            "loss: 0.901119589805603\n",
            "loss: 0.9371111392974854\n",
            "loss: 0.9873965382575989\n",
            "loss: 0.981277585029602\n",
            "loss: 1.1407487392425537\n",
            "loss: 1.0870100259780884\n",
            "loss: 0.7826890349388123\n",
            "loss: 0.7335066795349121\n",
            "loss: 0.9130977988243103\n",
            "loss: 0.7964051961898804\n",
            "loss: 0.8766756057739258\n",
            "loss: 0.8088940382003784\n",
            "loss: 0.9010071754455566\n",
            "loss: 1.061727523803711\n",
            "loss: 0.879173755645752\n",
            "loss: 0.9544318914413452\n",
            "loss: 0.8396642208099365\n",
            "loss: 0.7890029549598694\n",
            "loss: 0.7465885281562805\n",
            "loss: 0.7369312644004822\n",
            "loss: 0.898863673210144\n",
            "loss: 0.999435544013977\n",
            "loss: 0.9431138038635254\n",
            "loss: 0.8924805521965027\n",
            "loss: 0.9295310378074646\n",
            "loss: 0.871696412563324\n",
            "loss: 0.90714430809021\n",
            "loss: 1.072005033493042\n",
            "loss: 1.0435644388198853\n",
            "loss: 0.9516936540603638\n",
            "loss: 0.8065893650054932\n",
            "loss: 0.7478460669517517\n",
            "loss: 0.7319204211235046\n",
            "loss: 0.8641331791877747\n",
            "loss: 0.8296892642974854\n",
            "loss: 0.8246704339981079\n",
            "loss: 0.7731672525405884\n",
            "loss: 0.7676542401313782\n",
            "loss: 0.8933756351470947\n",
            "loss: 0.9439224004745483\n",
            "loss: 0.7910902500152588\n",
            "loss: 0.7659075856208801\n",
            "loss: 0.7888568639755249\n",
            "loss: 0.7024110555648804\n",
            "loss: 0.6863313913345337\n",
            "loss: 0.85941481590271\n",
            "loss: 0.9604239463806152\n",
            "loss: 0.8888846039772034\n",
            "loss: 0.8659828305244446\n",
            "loss: 0.730396032333374\n",
            "loss: 0.7788729667663574\n",
            "loss: 0.7929553985595703\n",
            "loss: 0.8324709534645081\n",
            "loss: 0.9688286185264587\n",
            "loss: 0.8144604563713074\n",
            "loss: 0.7289270162582397\n",
            "loss: 0.7906985878944397\n",
            "loss: 0.8571116328239441\n",
            "loss: 0.8533152937889099\n",
            "loss: 0.8338359594345093\n",
            "loss: 0.6167023181915283\n",
            "loss: 0.6412140130996704\n",
            "loss: 0.9312719702720642\n",
            "loss: 0.9071779251098633\n",
            "loss: 0.7862014174461365\n",
            "loss: 1.0342507362365723\n",
            "loss: 1.0482280254364014\n",
            "loss: 0.7974075675010681\n",
            "loss: 0.7933914661407471\n",
            "loss: 0.7068793773651123\n",
            "loss: 0.8634117841720581\n",
            "loss: 0.7632003426551819\n",
            "loss: 0.7310076951980591\n",
            "loss: 0.8172664642333984\n",
            "loss: 0.8823215961456299\n",
            "loss: 0.9183520674705505\n",
            "loss: 0.8339729309082031\n",
            "loss: 0.8117830157279968\n",
            "loss: 0.7048966288566589\n",
            "loss: 0.9522558450698853\n",
            "loss: 0.79182368516922\n",
            "loss: 0.8688979148864746\n",
            "loss: 0.876257061958313\n",
            "loss: 0.9770371317863464\n",
            "loss: 0.8708915114402771\n",
            "loss: 0.7705957889556885\n",
            "loss: 0.8184007406234741\n",
            "loss: 0.8766096830368042\n",
            "loss: 0.6685659885406494\n",
            "loss: 0.7222359776496887\n",
            "loss: 0.6988922357559204\n",
            "loss: 0.8275684118270874\n",
            "loss: 0.8544431328773499\n",
            "loss: 0.7155606746673584\n",
            "loss: 0.7365221381187439\n",
            "loss: 0.8669644594192505\n",
            "loss: 0.9661917090415955\n",
            "loss: 0.9021498560905457\n",
            "loss: 0.9296585917472839\n",
            "loss: 0.829593300819397\n",
            "loss: 0.794035792350769\n",
            "loss: 0.8037517070770264\n",
            "loss: 0.8909001350402832\n",
            "loss: 0.8364115357398987\n",
            "loss: 0.8675958514213562\n",
            "loss: 0.7267625331878662\n",
            "loss: 0.8465160131454468\n",
            "loss: 0.843480110168457\n",
            "loss: 0.6618428826332092\n",
            "loss: 0.6807205080986023\n",
            "loss: 0.7806809544563293\n",
            "loss: 0.8125232458114624\n",
            "loss: 0.760369062423706\n",
            "loss: 0.8175381422042847\n",
            "loss: 0.7963887453079224\n",
            "loss: 0.7027979493141174\n",
            "loss: 0.791923999786377\n",
            "loss: 1.004909873008728\n",
            "loss: 0.7877143621444702\n",
            "loss: 0.6908665895462036\n",
            "loss: 0.7232614755630493\n",
            "loss: 0.8265502452850342\n",
            "loss: 0.9747487902641296\n",
            "loss: 1.0750682353973389\n",
            "loss: 0.9334900975227356\n",
            "loss: 1.1115801334381104\n",
            "loss: 0.7820314764976501\n",
            "loss: 0.825442910194397\n",
            "loss: 0.9371446967124939\n",
            "loss: 1.0160408020019531\n",
            "loss: 0.8955312967300415\n",
            "loss: 0.8780934810638428\n",
            "loss: 0.8032615780830383\n",
            "loss: 0.7938340902328491\n",
            "loss: 1.050613284111023\n",
            "loss: 0.9256044030189514\n",
            "loss: 0.8410360813140869\n",
            "loss: 1.0464648008346558\n",
            "loss: 0.8679183125495911\n",
            "loss: 0.8778747320175171\n",
            "loss: 0.8342912197113037\n",
            "loss: 0.9767683744430542\n",
            "loss: 0.9888083934783936\n",
            "loss: 1.0575613975524902\n",
            "loss: 0.7822207808494568\n",
            "loss: 0.8347244262695312\n",
            "loss: 0.8119361996650696\n",
            "loss: 0.7782241106033325\n",
            "loss: 0.8667729496955872\n",
            "loss: 0.8415259122848511\n",
            "loss: 0.8833580613136292\n",
            "loss: 0.8282913565635681\n",
            "loss: 0.7576459050178528\n",
            "loss: 0.7619280815124512\n",
            "loss: 0.6195529103279114\n",
            "loss: 0.7317947745323181\n",
            "loss: 0.8192557096481323\n",
            "loss: 0.6854525208473206\n",
            "loss: 0.7743926644325256\n",
            "loss: 0.8730975389480591\n",
            "loss: 0.8184580206871033\n",
            "loss: 0.8556478023529053\n",
            "loss: 0.9372997879981995\n",
            "loss: 0.7431161403656006\n",
            "loss: 0.8602049350738525\n",
            "loss: 0.8253586292266846\n",
            "loss: 0.7545949220657349\n",
            "loss: 0.7852124571800232\n",
            "loss: 0.8651445508003235\n",
            "loss: 0.7149779796600342\n",
            "loss: 0.8623077869415283\n",
            "loss: 1.0131934881210327\n",
            "loss: 0.7619577646255493\n",
            "loss: 0.6594667434692383\n",
            "loss: 0.8565738201141357\n",
            "loss: 0.7883830070495605\n",
            "loss: 0.6376948356628418\n",
            "loss: 0.776078462600708\n",
            "loss: 0.7133752703666687\n",
            "loss: 0.8056146502494812\n",
            "loss: 0.7289596796035767\n",
            "loss: 0.8676831722259521\n",
            "loss: 0.9234187602996826\n",
            "loss: 0.8574895262718201\n",
            "loss: 0.8055288195610046\n",
            "loss: 0.8435356020927429\n",
            "loss: 0.7886297106742859\n",
            "loss: 0.8127532005310059\n",
            "loss: 0.8042536973953247\n",
            "loss: 0.8701257705688477\n",
            "loss: 0.7514504194259644\n",
            "loss: 0.8953135013580322\n",
            "loss: 0.6904440522193909\n",
            "loss: 0.7035983800888062\n",
            "loss: 0.6924282908439636\n",
            "loss: 0.6140007972717285\n",
            "loss: 0.699932873249054\n",
            "loss: 0.962955892086029\n",
            "loss: 0.8397120833396912\n",
            "loss: 0.7794204354286194\n",
            "loss: 0.9866490364074707\n",
            "loss: 0.8955965638160706\n",
            "loss: 0.7743720412254333\n",
            "loss: 0.7444829940795898\n",
            "loss: 0.790393590927124\n",
            "loss: 0.7416322827339172\n",
            "loss: 0.8326448202133179\n",
            "loss: 0.8070544004440308\n",
            "loss: 0.7723662853240967\n",
            "loss: 0.6787508726119995\n",
            "loss: 0.8313356041908264\n",
            "loss: 0.7903644442558289\n",
            "loss: 0.932067334651947\n",
            "loss: 1.0141191482543945\n",
            "loss: 0.9551036357879639\n",
            "loss: 0.8074718117713928\n",
            "loss: 0.897746205329895\n",
            "loss: 0.7693430185317993\n",
            "loss: 0.7279176712036133\n",
            "loss: 0.9560940861701965\n",
            "loss: 0.9875718355178833\n",
            "loss: 0.8769214153289795\n",
            "loss: 0.7582070231437683\n",
            "loss: 0.7598530054092407\n",
            "loss: 0.7184566855430603\n",
            "loss: 0.933772087097168\n",
            "loss: 1.011123538017273\n",
            "loss: 0.7209811210632324\n",
            "loss: 0.5885212421417236\n",
            "loss: 0.6713485717773438\n",
            "loss: 0.6739261150360107\n",
            "loss: 0.7402523756027222\n",
            "loss: 0.8547030091285706\n",
            "loss: 0.8420321941375732\n",
            "loss: 0.8877452611923218\n",
            "loss: 0.9154523015022278\n",
            "loss: 0.9219547510147095\n",
            "loss: 0.7032095193862915\n",
            "loss: 0.6250334978103638\n",
            "loss: 0.7715568542480469\n",
            "loss: 0.8973743915557861\n",
            "loss: 0.8044489622116089\n",
            "loss: 0.8407530784606934\n",
            "loss: 0.6927177906036377\n",
            "loss: 0.7383551597595215\n",
            "loss: 0.9027007818222046\n",
            "loss: 0.7770448923110962\n",
            "loss: 0.8076640963554382\n",
            "loss: 0.9028973579406738\n",
            "loss: 0.9114813208580017\n",
            "loss: 0.798187255859375\n",
            "loss: 0.909299373626709\n",
            "loss: 0.8670134544372559\n",
            "loss: 0.6753754019737244\n",
            "loss: 0.6973636150360107\n",
            "loss: 0.6871234774589539\n",
            "loss: 0.6707140803337097\n",
            "loss: 0.7550375461578369\n",
            "loss: 0.7751271724700928\n",
            "loss: 0.9185233116149902\n",
            "loss: 0.8351578116416931\n",
            "loss: 0.8286599516868591\n",
            "loss: 0.7200322151184082\n",
            "loss: 0.6305202841758728\n",
            "loss: 0.8464665412902832\n",
            "loss: 0.6440861821174622\n",
            "loss: 0.5726363658905029\n",
            "loss: 0.7416821718215942\n",
            "loss: 0.6356091499328613\n",
            "loss: 0.7134367823600769\n",
            "loss: 0.6866069436073303\n",
            "loss: 0.6746174097061157\n",
            "loss: 1.0251694917678833\n",
            "loss: 0.8727589845657349\n",
            "loss: 0.8563759922981262\n",
            "loss: 0.6223750114440918\n",
            "loss: 0.9517828822135925\n",
            "loss: 1.0388555526733398\n",
            "loss: 0.7890571355819702\n",
            "loss: 0.8864659070968628\n",
            "loss: 0.884850025177002\n",
            "loss: 0.7232397198677063\n",
            "loss: 0.8006201386451721\n",
            "loss: 1.0486037731170654\n",
            "loss: 0.7676461935043335\n",
            "loss: 0.820636510848999\n",
            "loss: 0.815761387348175\n",
            "loss: 0.7031649351119995\n",
            "loss: 0.6357415318489075\n",
            "loss: 0.7719521522521973\n",
            "loss: 0.8272348046302795\n",
            "loss: 0.6916598081588745\n",
            "loss: 0.6353030204772949\n",
            "loss: 0.6800434589385986\n",
            "loss: 0.7574433088302612\n",
            "loss: 0.7501334547996521\n",
            "loss: 0.824548065662384\n",
            "loss: 1.0318204164505005\n",
            "loss: 0.8157769441604614\n",
            "loss: 0.679287314414978\n",
            "loss: 0.5645897388458252\n",
            "loss: 0.7665688991546631\n",
            "loss: 0.866350531578064\n",
            "loss: 0.9371781349182129\n",
            "loss: 0.7113450169563293\n",
            "loss: 0.7828547358512878\n",
            "loss: 0.677562415599823\n",
            "loss: 0.7007248401641846\n",
            "loss: 0.8542686700820923\n",
            "loss: 0.7573288083076477\n",
            "loss: 0.6810458302497864\n",
            "loss: 0.8500633239746094\n",
            "loss: 0.8449455499649048\n",
            "loss: 0.6898307800292969\n",
            "loss: 0.7103400230407715\n",
            "loss: 0.6841509342193604\n",
            "loss: 0.7104833126068115\n",
            "loss: 0.6051434278488159\n",
            "loss: 0.8016639351844788\n",
            "loss: 0.932784914970398\n",
            "loss: 0.6033782958984375\n",
            "loss: 0.6626994609832764\n",
            "loss: 0.6907758712768555\n",
            "loss: 0.7545132637023926\n",
            "loss: 0.664209246635437\n",
            "loss: 0.6362621188163757\n",
            "loss: 0.6595674157142639\n",
            "loss: 0.578987717628479\n",
            "loss: 0.6902785897254944\n",
            "loss: 0.7995932698249817\n",
            "loss: 0.6939456462860107\n",
            "loss: 0.7664620876312256\n",
            "loss: 0.7785074710845947\n",
            "loss: 0.6503676772117615\n",
            "loss: 0.6564193367958069\n",
            "loss: 0.7767977714538574\n",
            "loss: 0.6439995169639587\n",
            "loss: 0.8424960970878601\n",
            "loss: 0.7012625336647034\n",
            "loss: 0.8308854103088379\n",
            "loss: 0.7899758815765381\n",
            "loss: 0.7578834295272827\n",
            "loss: 0.7083961963653564\n",
            "loss: 0.6911999583244324\n",
            "loss: 0.5515111684799194\n",
            "loss: 0.6361844539642334\n",
            "loss: 0.6467570066452026\n",
            "loss: 0.6701927185058594\n",
            "loss: 0.7029172778129578\n",
            "loss: 0.6841683387756348\n",
            "loss: 0.4305352568626404\n",
            "loss: 0.49353495240211487\n",
            "loss: 0.5210211277008057\n",
            "loss: 0.8195486068725586\n",
            "loss: 0.5749109387397766\n",
            "loss: 0.547335147857666\n",
            "loss: 0.806238055229187\n",
            "loss: 0.45335257053375244\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9ra74jZkPJa",
        "outputId": "37e1a19b-3903-46f4-b231-5cc56cec0045"
      },
      "source": [
        "batches, acc = 0, 0\n",
        "for x_batch, y_batch in test_loader:\n",
        "  \n",
        "  batches += 1\n",
        "  x_batch = x_batch.reshape(x_batch.shape[0], -1)\n",
        "\n",
        "  preds = torch.argmax(x_batch @ w, dim = 1)\n",
        "  acc += (preds == y_batch).numpy().mean()\n",
        "  result = acc/batches\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8475560897435898\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJT6j_BEp38h"
      },
      "source": [
        "from torch import nn\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer  = torch.optim.Adam(model.parameters())\n",
        "model  = nn.Sequential(\n",
        "    nn.Linear(features, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, classes)\n",
        ")\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "   \n",
        "    for x_batch, y_batch in train_loader:\n",
        "     \n",
        "       x_batch = x_batch.reshape(x_batch.shape[0],-1)\n",
        "       logits  = model (x_batch)\n",
        "       loss = criterion(logits, y_batch)\n",
        "       history.append(loss.item())\n",
        "       optimizer.zero_grad()\n",
        "       loss.backward()\n",
        "       optimizer.step()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}